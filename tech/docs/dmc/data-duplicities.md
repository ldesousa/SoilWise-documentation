# Data duplicities discovery

- two levels inspection (coarse = dataset level, fine = objects/attributes? level)
- read existing data in terms of size, identical identifiers (data, metadata level)
- identify duplicit values

- connections with: catalogue, (meta)data input, validation + monitoring + ets, scheme and structure, storage, workflows?
- technologies used:
- responsible person: